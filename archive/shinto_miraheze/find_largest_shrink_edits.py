#!/usr/bin/env python3
"""find_largest_shrink_edits.py
================================================
Find the largest shrink (deletion) edit for each page in
[[Category:Autogenerated pages with jawiki interwikis, possibly accidentally overwritten]]

This analyzes edit history to identify potential overwrites.
"""

import mwclient
import sys
import time

if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

WIKI_URL  = 'shinto.miraheze.org'
WIKI_PATH = '/w/'
USERNAME  = 'Immanuelle'
PASSWORD  = '[REDACTED_SECRET_2]'

site = mwclient.Site(WIKI_URL, path=WIKI_PATH)
site.login(USERNAME, PASSWORD)

try:
    ui = site.api('query', meta='userinfo')
    logged_user = ui['query']['userinfo'].get('name', USERNAME)
    print(f"Logged in as {logged_user}\n")
except Exception:
    print("Logged in (could not fetch username via API)\n")

def get_page_revisions(page_title):
    """Get revision history for a page, including size info."""
    try:
        revisions = site.api('query',
                            titles=page_title,
                            prop='revisions',
                            rvprop='timestamp|user|comment|size',
                            rvlimit='max')

        pages = revisions.get('query', {}).get('pages', {})
        if not pages:
            return []

        page_data = list(pages.values())[0]
        return page_data.get('revisions', [])
    except Exception as e:
        print(f"   ! Error fetching revisions for {page_title}: {e}")
        return []

def find_largest_shrink(revisions):
    """Find the largest shrink edit in revision history."""
    if not revisions or len(revisions) < 2:
        return None

    largest_shrink = None
    largest_shrink_amount = 0

    # Sort revisions by timestamp (oldest first)
    sorted_revs = sorted(revisions, key=lambda r: r.get('timestamp', ''), reverse=True)

    # Look for negative changes (shrinks)
    for i in range(len(sorted_revs) - 1):
        current = sorted_revs[i]
        previous = sorted_revs[i + 1]

        current_size = current.get('size', 0)
        previous_size = previous.get('size', 0)

        shrink_amount = previous_size - current_size

        if shrink_amount > largest_shrink_amount:
            largest_shrink_amount = shrink_amount
            largest_shrink = {
                'revision': current,
                'shrink_amount': shrink_amount,
                'before_size': previous_size,
                'after_size': current_size,
                'user': current.get('user', 'Unknown'),
                'comment': current.get('comment', ''),
                'timestamp': current.get('timestamp', ''),
            }

    return largest_shrink

def main():
    """Find largest shrink edits for all pages in the category."""

    print("Finding largest shrink edits for pages in [[Category:Autogenerated pages with jawiki interwikis, possibly accidentally overwritten]]\n")
    print("=" * 80)

    category = site.pages['Category:Autogenerated pages with jawiki interwikis, possibly accidentally overwritten']

    try:
        all_members = list(category.members())
        members = [m for m in all_members if m.namespace == 0]
    except Exception as e:
        print(f"ERROR: Could not fetch category members – {e}")
        return

    print(f"Found {len(members)} mainspace pages\n")

    results = []

    for i, page in enumerate(members, 1):
        page_name = page.name
        print(f"{i:3d}. {page_name:50s}", end="", flush=True)

        revisions = get_page_revisions(page_name)

        if not revisions:
            print(" [NO REVISIONS FOUND]")
            continue

        shrink = find_largest_shrink(revisions)

        if shrink:
            print(f" [{shrink['shrink_amount']:6d} bytes removed]")
            results.append({
                'page': page_name,
                'shrink_amount': shrink['shrink_amount'],
                'before_size': shrink['before_size'],
                'after_size': shrink['after_size'],
                'user': shrink['user'],
                'comment': shrink['comment'],
                'timestamp': shrink['timestamp'],
            })
        else:
            print(" [NO SHRINKS FOUND]")

        time.sleep(0.5)

    # Sort results by shrink amount (largest first)
    results.sort(key=lambda r: r['shrink_amount'], reverse=True)

    # Save to file
    output_file = 'largest_shrink_edits.txt'
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("Largest shrink edits by page\n")
        f.write("=" * 80 + "\n\n")

        for result in results:
            f.write(f"Page: {result['page']}\n")
            f.write(f"  Shrink amount: {result['shrink_amount']} bytes\n")
            f.write(f"  Before: {result['before_size']} bytes → After: {result['after_size']} bytes\n")
            f.write(f"  User: {result['user']}\n")
            f.write(f"  Timestamp: {result['timestamp']}\n")
            f.write(f"  Comment: {result['comment']}\n")
            f.write("\n")

    print(f"\n{'=' * 80}")
    print(f"Results saved to: {output_file}")
    print(f"\nTop 10 largest shrinks:")
    print("-" * 80)
    for i, result in enumerate(results[:10], 1):
        print(f"{i:2d}. {result['page']:50s} {result['shrink_amount']:6d} bytes")

if __name__ == "__main__":
    main()
